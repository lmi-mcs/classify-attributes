import pandas as pd
import openai
import json
import time
import os
from datetime import datetime
from typing import Dict, List
import argparse
from dotenv import load_dotenv

# Classification categories
CLASSIFICATION_CATEGORIES = {
    "çµ„ç¹”æ§‹é€ ": [
        "éƒ¨ç½²ãƒ»äº‹æ¥­éƒ¨ï¼ˆçµŒç†èª²ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã€æµ·å¤–äº‹æ¥­éƒ¨ã€æœ¬éƒ¨ã€éƒ¨ã€èª²ã€å®¤ã€ä¿‚ã€ç­ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ¦ãƒ‹ãƒƒãƒˆãªã©ï¼‰",
        "ãƒãƒ¼ãƒ ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒ ã€ãƒ‡ã‚£ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³Gã€Webã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°Gã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ¼ãƒ ãªã©ï¼‰",
        "æ‹ ç‚¹ãƒ»ã‚¨ãƒªã‚¢ï¼ˆæ–°å®¿åº—ã€æ¢…ç”°åº—ã€æ±æµ·ã‚¨ãƒªã‚¢ã€å°æ¹¾æ”¯ç¤¾ã€æ”¯åº—ã€å–¶æ¥­æ‰€ã€å·¥å ´ã€ã‚»ãƒ³ã‚¿ãƒ¼ã€äº‹æ¥­æ‰€ã€åº—èˆ—ãªã©ï¼‰",
        "åœ°åŸŸãƒ»ã‚¨ãƒªã‚¢ï¼ˆé–¢æ±ã€æ±æµ·ã€é–¢è¥¿ã€æµ·å¤–ãªã©ï¼‰",
        "çµ„ç¹”éšå±¤ï¼ˆçµ„ç¹”1ã€çµ„ç¹”2ã€çµ„ç¹”éšå±¤ï¼‘ãªã©ï¼‰",
    ],
    "äººäº‹ç®¡ç†": [
        "è·ä½éšå±¤ï¼ˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€èª²é•·ã€éƒ¨é•·ã€ãƒªãƒ¼ãƒ€ãƒ¼ã€ãƒ¡ãƒ³ãƒãƒ¼ã€ç®¡ç†è·ã€å½¹è·ã€è·ä½ã€Positionãªã©ï¼‰",
        "äººäº‹ç­‰ç´šï¼ˆM1ã€ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã‚¯ãƒ©ã‚¹ã€ç­‰ç´šã€Gradeã€ã‚¸ãƒ§ãƒ–ã‚°ãƒ¬ãƒ¼ãƒ‰ã€è·ç´šã€ãƒ©ãƒ³ã‚¯ãªã©ï¼‰",
        "è·è²¬ãƒ»å½¹å‰²ï¼ˆç¤¾å“¡ã€è·è²¬ã€ãƒ—ãƒ¬ã‚¤ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€å½¹å‰²ã€è·åã€è·å‹™ã€è·æŒã€è·åˆ†ãªã©ï¼‰",
        "é…å±ãƒ»æ‰€å±ï¼ˆæ‰€å±ã€é…å±ã€æ‰€å±éƒ¨ç½²ã€æ‰€å±éƒ¨é–€ã€æ‰€å±ãƒãƒ¼ãƒ ãªã©ï¼‰",
    ],
    "é›‡ç”¨ç®¡ç†": [
        "é›‡ç”¨åŒºåˆ†ï¼ˆæ­£ç¤¾å“¡ã€å¥‘ç´„ç¤¾å“¡ã€æ´¾é£ã€ã‚¢ãƒ«ãƒã‚¤ãƒˆã€å†…å®šç¤¾å“¡ã€é›‡ç”¨å½¢æ…‹ã€é›‡ç”¨åŒºåˆ†ã€ç¤¾å“¡åŒºåˆ†ã€å¾“æ¥­å“¡åŒºåˆ†ãªã©ï¼‰",
        "æ¡ç”¨å½¢æ…‹ï¼ˆæ–°å’ã€ä¸­é€”å…¥ç¤¾ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³æ¡ç”¨ã€æ¡ç”¨ã€æ¡ç”¨åŒºåˆ†ã€æ¡ç”¨å½¢æ…‹ã€æ¡ç”¨ç¨®åˆ¥ã€å…¥ç¤¾çµŒè·¯ã€å…¥ç¤¾çµŒç·¯ãªã©ï¼‰",
        "å‹¤å‹™æœŸé–“ï¼ˆå…¥ç¤¾æ™‚æœŸã€å‹¤ç¶šå¹´æ•°ã€å°±æ¥­å¹´æ•°ã€å…¥ç¤¾å¹´ã€å…¥ç¤¾å¹´åº¦ã€å…¥ç¤¾å¹´æ¬¡ã€å…¥ç¤¾åŒºåˆ†ã€ç¤¾æ­´ã€åœ¨ç±å¹´æ•°ã€åœ¨è·å¹´æ•°ã€å‹¤å‹™å¹´æ•°ã€å…¥ç¤¾å¹´æ•°ã€ç¾ä¼šç¤¾ã§ã®å°±æ¥­å¹´æ•°ã€åœ¨ç±æœŸé–“ã€å¹´æ¬¡ãªã©ï¼‰",
        "å‹¤å‹™å½¢æ…‹ï¼ˆå‹¤å‹™å½¢æ…‹ã€å‹¤å‹™åŒºåˆ†ã€å°±æ¥­å½¢æ…‹ã€å†…å¤–å‹¤ã€å‡ºå‘åŒºåˆ†ã€å‡ºå‘å…ˆãªã©ï¼‰",
    ],
    "æ¥­å‹™æ©Ÿèƒ½": [
        "è·ç¨®åˆ†é¡ï¼ˆæŠ€è¡“ã€å–¶æ¥­ã€äº‹å‹™ã€ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆã€è·ç¨®ã€è·ç¾¤ã€è·å‹™åŒºåˆ†ã€è·æŒãƒ»è³‡æ ¼ãªã©ï¼‰",
        "å°‚é–€é ˜åŸŸï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã€ã‚·ã‚¹ãƒ†ãƒ ã€ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€è·èƒ½è³‡æ ¼ãªã©ï¼‰",
    ],
    "å€‹äººå±æ€§": [
        "åŸºæœ¬æƒ…å ±ï¼ˆå¹´é½¢ã€æ€§åˆ¥ã€å¹´ä»£ã€å¹´é½¢å±¤ã€å¹´é½¢åŒºåˆ†ã€ç”Ÿå¹´ã€ç”·å¥³ã€Genderã€Ageã€å©šå§»ã€å®¶æ—æ§‹æˆã€ä¸–ä»£ãªã©ï¼‰",
        "åœ°åŸŸãƒ»ã‚¨ãƒªã‚¢ï¼ˆæ±æµ·ã‚¨ãƒªã‚¢ã€é–¢æ±ã€åœ°åŒºã€åœ°åŸŸã€å¤–å›½ç±ã€å›½ç±ãªã©ï¼‰",
        "å­¦æ­´ï¼ˆå­¦æ­´ã€æœ€çµ‚å­¦æ­´ãªã©ï¼‰",
    ],
    "ãã®ä»–ãƒ»æœªåˆ†é¡": ["ç‰¹æ®Šã‚«ãƒ†ã‚´ãƒªãƒ»nullå€¤ãªã©"],
}


def create_classification_prompt_with_confidence(attribute_names: List[str]) -> str:
    """Create a prompt for classifying Japanese attribute group names with confidence scores."""

    categories_text = ""
    for main_category, subcategories in CLASSIFICATION_CATEGORIES.items():
        categories_text += f"\n{main_category}:\n"
        for subcategory in subcategories:
            categories_text += f"  - {subcategory}\n"

    # ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã®ãƒªã‚¹ãƒˆã‚’æ˜ç¢ºã«æç¤º
    main_categories = list(CLASSIFICATION_CATEGORIES.keys())
    main_categories_text = "ã€".join(main_categories)

    prompt = f"""ã‚ãªãŸã¯æ—¥æœ¬ã®å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—åã‚’äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’æ‹…å½“ã—ã¦ã„ã¾ã™ã€‚

## é‡è¦: å¿…ãšä»¥ä¸‹ã®6ã¤ã®ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{main_categories_text}

åˆ†é¡ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°:
{categories_text}

## åˆ†é¡ãƒ«ãƒ¼ãƒ«:
1. **å¿…é ˆ**: å„å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—åã‚’ä¸Šè¨˜ã®6ã¤ã®ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã«åˆ†é¡ã—ã¦ãã ã•ã„
2. **ç¦æ­¢**: ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚„èª¬æ˜æ–‡ã®ä¸€éƒ¨ï¼ˆä¾‹ï¼šã€Œéƒ¨ç½²ãƒ»äº‹æ¥­éƒ¨ã€ã€Œãƒãƒ¼ãƒ ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ã€ãªã©ï¼‰ã‚’è¿”ã•ãªã„ã§ãã ã•ã„
3. æ—¥æœ¬èªã®æ„å‘³ã¨æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦ãã ã•ã„
4. ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š
   - å…¥ç¤¾ã€æ¡ç”¨ã€é›‡ç”¨ã€å‹¤å‹™ã€å°±æ¥­ã€å¥‘ç´„ã€æ­£ç¤¾å“¡ã€æ´¾é£ â†’ é›‡ç”¨ç®¡ç†
   - éƒ¨ç½²ã€éƒ¨ã€èª²ã€å®¤ã€ä¿‚ã€ãƒãƒ¼ãƒ ã€ã‚°ãƒ«ãƒ¼ãƒ—ã€æ‹ ç‚¹ã€æ”¯åº—ã€åº—èˆ—ã€çµ„ç¹” â†’ çµ„ç¹”æ§‹é€ 
   - å½¹è·ã€è·ä½ã€ç­‰ç´šã€ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã‚¯ãƒ©ã‚¹ã€è·è²¬ã€è·å‹™ã€ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ â†’ äººäº‹ç®¡ç†
   - å¹´é½¢ã€æ€§åˆ¥ã€å¹´ä»£ã€å­¦æ­´ã€åœ°åŸŸã€å›½ç± â†’ å€‹äººå±æ€§
   - è·ç¨®ã€è·ç¾¤ã€å°‚é–€ã€æŠ€è¡“ã€å–¶æ¥­ â†’ æ¥­å‹™æ©Ÿèƒ½
   - ä¸Šè¨˜ã«å½“ã¦ã¯ã¾ã‚‰ãªã„å ´åˆ â†’ ãã®ä»–ãƒ»æœªåˆ†é¡

## åˆ†é¡ä¾‹ï¼ˆæ­£ã—ã„å½¢å¼ï¼‰:
- å…¥ç¤¾åŒºåˆ† â†’ é›‡ç”¨ç®¡ç† (ä¿¡é ¼åº¦: 0.9)
- ãƒ‡ãƒ¼ã‚¿Uç·¨é›†éƒ¨ â†’ çµ„ç¹”æ§‹é€  (ä¿¡é ¼åº¦: 0.8)
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒ  â†’ çµ„ç¹”æ§‹é€  (ä¿¡é ¼åº¦: 0.9)
- è·ä½ â†’ äººäº‹ç®¡ç† (ä¿¡é ¼åº¦: 0.9)
- æ‰€å± â†’ äººäº‹ç®¡ç† (ä¿¡é ¼åº¦: 0.7)
- å‹¤å‹™åœ° â†’ çµ„ç¹”æ§‹é€  (ä¿¡é ¼åº¦: 0.8)
- ä¼šç¤¾å â†’ ãã®ä»–ãƒ»æœªåˆ†é¡ (ä¿¡é ¼åº¦: 0.3)
- å›½ç± â†’ å€‹äººå±æ€§ (ä¿¡é ¼åº¦: 0.8)

## ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢åŸºæº–:
- 0.9-1.0: éå¸¸ã«ç¢ºä¿¡ï¼ˆæ˜ç¢ºã§æ›–æ˜§ã•ã®ãªã„åˆ†é¡ï¼‰
- 0.7-0.8: ç¢ºä¿¡ï¼ˆãŠãã‚‰ãæ­£ã—ã„ãŒè‹¥å¹²ã®æ›–æ˜§ã•ã‚ã‚Šï¼‰
- 0.5-0.6: ä¸­ç¨‹åº¦ã®ç¢ºä¿¡ï¼ˆå¦¥å½“ãªåˆ†é¡ã ãŒä¸ç¢ºå®Ÿï¼‰
- 0.3-0.4: ä½ã„ç¢ºä¿¡ï¼ˆåˆ†é¡ãŒå›°é›£ã€è¤‡æ•°ã®å¯èƒ½æ€§ï¼‰
- 0.0-0.2: éå¸¸ã«ä½ã„ç¢ºä¿¡ï¼ˆä¸æ˜ç¢ºã¾ãŸã¯æ›–æ˜§ï¼‰

## å‡ºåŠ›å½¢å¼:
ä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚categoryã«ã¯å¿…ãšä¸Šè¨˜6ã¤ã®ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š

{{
  "å±æ€§å": {{
    "category": "çµ„ç¹”æ§‹é€ ",
    "confidence": 0.85
  }},
  ...
}}

åˆ†é¡å¯¾è±¡ã®å±æ€§å:
{json.dumps(attribute_names, ensure_ascii=False, indent=2)}

é‡è¦: JSONåˆ†é¡çµæœã®ã¿ã‚’è¿”ã—ã€è¿½åŠ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚„èª¬æ˜ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚å¿…ãš6ã¤ã®ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"""

    return prompt


def classify_with_openai_confidence(
    client: openai.OpenAI, attribute_names: List[str]
) -> Dict[str, Dict[str, float]]:
    """Classify attribute names using OpenAI API with confidence scores."""

    prompt = create_classification_prompt_with_confidence(attribute_names)

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=4000,
        )

        text_content = response.choices[0].message.content

        if text_content is None:
            print("APIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ")
            return {}

        # Remove markdown code block formatting if present
        if text_content.startswith("```json"):
            text_content = text_content.replace("```json\n", "").replace("\n```", "")
        elif text_content.startswith("```"):
            text_content = text_content.replace("```\n", "").replace("\n```", "")

        result = json.loads(text_content)
        return result

    except Exception as e:
        print(f"Error with OpenAI API (confidence mode): {str(e)}")
        return {}


def process_csv_with_confidence(
    client: openai.OpenAI,
    df: pd.DataFrame,
    batch_size: int = 50,
    use_confidence: bool = True,
) -> pd.DataFrame:
    """Process CSV data in batches with optional confidence scores."""

    results = {}
    attribute_names = df["å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å"].tolist()

    print(f"Total attributes to classify: {len(attribute_names)}")
    print(f"Processing in batches of {batch_size}...")

    # Process in batches
    for i in range(0, len(attribute_names), batch_size):
        batch = attribute_names[i : i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(attribute_names) + batch_size - 1) // batch_size

        print(f"Processing batch {batch_num}/{total_batches}...")

        batch_results = classify_with_openai_confidence(client, batch)

        results.update(batch_results)

        # Rate limiting - wait between requests
        if i + batch_size < len(attribute_names):
            time.sleep(0.2)

    # Add classification results to dataframe
    if use_confidence:
        df["åˆ†é¡"] = df["å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å"].map(
            lambda x: results.get(x, {}).get("category", "ãã®ä»–ãƒ»æœªåˆ†é¡")
        )
        df["ä¿¡é ¼åº¦"] = df["å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å"].map(
            lambda x: results.get(x, {}).get("confidence", 0.0)
        )
    else:
        df["åˆ†é¡"] = df["å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å"].map(
            lambda x: results.get(x, "ãã®ä»–ãƒ»æœªåˆ†é¡")
        )

    return df


def extract_low_confidence_items(
    df: pd.DataFrame, threshold: float = 0.7
) -> pd.DataFrame:
    """Extract items with low confidence scores for priority validation."""
    if "ä¿¡é ¼åº¦" not in df.columns:
        print("ä¿¡é ¼åº¦åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    low_confidence = df[df["ä¿¡é ¼åº¦"] < threshold].copy()
    return low_confidence.sort_values(by=["ä¿¡é ¼åº¦"], ascending=[True])  # type: ignore


def analyze_confidence_distribution(df: pd.DataFrame) -> Dict:
    """Analyze and return confidence score distribution as dictionary."""
    if "ä¿¡é ¼åº¦" not in df.columns:
        return {}

    stats = {
        "å¹³å‡ä¿¡é ¼åº¦": df["ä¿¡é ¼åº¦"].mean(),
        "ä¸­å¤®å€¤": df["ä¿¡é ¼åº¦"].median(),
        "æœ€å°å€¤": df["ä¿¡é ¼åº¦"].min(),
        "æœ€å¤§å€¤": df["ä¿¡é ¼åº¦"].max(),
    }

    # ä¿¡é ¼åº¦åŒºé–“åˆ¥ã®çµ±è¨ˆ
    confidence_ranges = [
        ("Very High (0.9-1.0)", 0.9, 1.0),
        ("High (0.7-0.9)", 0.7, 0.9),
        ("Medium (0.5-0.7)", 0.5, 0.7),
        ("Low (0.3-0.5)", 0.3, 0.5),
        ("Very Low (0.0-0.3)", 0.0, 0.3),
    ]

    range_stats = []
    for label, min_val, max_val in confidence_ranges:
        count = len(df[(df["ä¿¡é ¼åº¦"] >= min_val) & (df["ä¿¡é ¼åº¦"] < max_val)])
        percentage = count / len(df) * 100
        range_stats.append({"ä¿¡é ¼åº¦åŒºé–“": label, "ä»¶æ•°": count, "å‰²åˆ": percentage})

    return {"stats": stats, "ranges": range_stats}


def generate_markdown_report(
    df: pd.DataFrame,
    output_file: str,
    confidence_analysis: Dict,
    low_confidence_items: pd.DataFrame,
    processing_time: float = 0.0,
) -> str:
    """Generate a comprehensive markdown report."""

    timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")

    # åˆ†é¡çµæœã®é›†è¨ˆ
    classification_counts = df["åˆ†é¡"].value_counts()

    # ä¿¡é ¼åº¦åˆ†æ
    confidence_stats = confidence_analysis.get("stats", {})
    confidence_ranges = confidence_analysis.get("ranges", [])

    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    report = f"""# å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡çµæœãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {timestamp}  
**å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: {output_file}  

## åˆ†é¡çµæœã‚µãƒãƒªãƒ¼

### åˆ†é¡åˆ¥ä»¶æ•°

| åˆ†é¡ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° | å‰²åˆ |
|-------------|------|------|"""

    for category, count in classification_counts.items():
        percentage = count / len(df) * 100
        report += f"\n| {category} | {count:,}ä»¶ | {percentage:.1f}% |"

    report += """

## ä¿¡é ¼åº¦åˆ†æ

### ä¿¡é ¼åº¦çµ±è¨ˆ

| é …ç›® | å€¤ |
|------|-----|"""

    for key, value in confidence_stats.items():
        report += f"\n| {key} | {value:.3f} |"

    report += """

### ä¿¡é ¼åº¦åŒºé–“åˆ¥çµ±è¨ˆ

| ä¿¡é ¼åº¦åŒºé–“ | ä»¶æ•° | å‰²åˆ |
|-----------|------|------|"""

    for range_info in confidence_ranges:
        report += f"\n| {range_info['ä¿¡é ¼åº¦åŒºé–“']} | {range_info['ä»¶æ•°']:,}ä»¶ | {range_info['å‰²åˆ']:.1f}% |"

    report += """

## ä½ä¿¡é ¼åº¦é …ç›®

**é–¾å€¤**: 0.7æœªæº€  

### ä¿¡é ¼åº¦ãŒæœ€ã‚‚ä½ã„é …ç›®ï¼ˆä¸Šä½10ä»¶ï¼‰

| é †ä½ | å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å | åˆ†é¡ | ä¿¡é ¼åº¦ |
|------|---------------|------|--------|"""

    top_low_confidence = low_confidence_items.head(10)
    for i, (_, row) in enumerate(top_low_confidence.iterrows(), 1):
        report += (
            f"\n| {i} | {row['å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å']} | {row['åˆ†é¡']} | {row['ä¿¡é ¼åº¦']:.3f} |"
        )

    report += """

## è©³ç´°ãƒ‡ãƒ¼ã‚¿

### åˆ†é¡åˆ¥è©³ç´°ä¸€è¦§

å„åˆ†é¡ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°ãªå±æ€§ã‚°ãƒ«ãƒ¼ãƒ—ä¸€è¦§ã¯ã€å‡ºåŠ›ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

### ãƒ‡ãƒ¼ã‚¿å½¢å¼

| åˆ—å | èª¬æ˜ |
|------|------|
| å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å | åˆ†é¡å¯¾è±¡ã®å±æ€§å |
| æ•° | è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ä»¶æ•° |
| åˆ†é¡ | è‡ªå‹•åˆ†é¡ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª |
| ä¿¡é ¼åº¦ | åˆ†é¡ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰ |

## æ¤œè¨¼æ¨å¥¨é …ç›®

ä»¥ä¸‹ã®é …ç›®ã¯ä¿¡é ¼åº¦ãŒä½ã„ãŸã‚ã€æ‰‹å‹•ã§ã®æ¤œè¨¼ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š

1. **ä¿¡é ¼åº¦0.3æœªæº€**: åˆ†é¡ãŒå›°é›£ãªé …ç›®
2. **ä¿¡é ¼åº¦0.3-0.5**: è¤‡æ•°ã®åˆ†é¡å¯èƒ½æ€§ãŒã‚ã‚‹é …ç›®
3. **ä¿¡é ¼åº¦0.5-0.7**: åŸºæœ¬çš„ãªåˆ†é¡ã¯å¯èƒ½ã ãŒã€è©³ç´°ãªæ¤œè¨¼ãŒå¿…è¦ãªé …ç›®

"""

    return report


def create_date_directory() -> str:
    """å½“æ—¥ã®æ—¥ä»˜ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã€ãã®ãƒ‘ã‚¹ã‚’è¿”ã™"""
    today = datetime.now().strftime("%Y%m%d")
    directory_path = os.path.join(".", today)

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)
        print(f"æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {directory_path}")

    return directory_path


def save_markdown_report(report: str, output_file: str, date_dir: str) -> str:
    """Save markdown report to file in the date directory."""
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ã—ã€æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®
    filename = os.path.basename(output_file).replace(".csv", "_report.md")
    report_file = os.path.join(date_dir, filename)

    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    return report_file


def select_csv_file():
    """Prompt user to select a CSV file."""
    print("åˆ†é¡å‰ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print(
        "ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å…¥åŠ›ã™ã‚‹ã‹ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º:"
    )

    user_input = input().strip()

    if user_input:
        if os.path.exists(user_input):
            return user_input
        else:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {user_input}")
            return None

    # List CSV files in current directory
    csv_files = [f for f in os.listdir(".") if f.endswith(".csv")]

    if not csv_files:
        print("ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    print("åˆ©ç”¨å¯èƒ½ãªCSVãƒ•ã‚¡ã‚¤ãƒ«:")
    for i, file in enumerate(csv_files, 1):
        print(f"{i}. {file}")

    while True:
        try:
            choice = input("ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
            if choice.isdigit():
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(csv_files):
                    return csv_files[choice_idx]
                else:
                    print("ç„¡åŠ¹ãªç•ªå·ã§ã™ã€‚")
            else:
                print("æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except KeyboardInterrupt:
            print("\nå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            return None


def main():
    parser = argparse.ArgumentParser(
        description="å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—è‡ªå‹•åˆ†é¡ãƒ„ãƒ¼ãƒ«ï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰"
    )
    parser.add_argument("--api-key", help="OpenAI API Key")
    parser.add_argument("--input", help="å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--output", help="å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument(
        "--batch-size", type=int, default=50, help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰"
    )
    parser.add_argument(
        "--no-confidence",
        action="store_true",
        help="ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã—ãªã„ï¼ˆå¾“æ¥ãƒ¢ãƒ¼ãƒ‰ï¼‰",
    )
    parser.add_argument(
        "--confidence-threshold",
        type=float,
        default=0.7,
        help="ä½ä¿¡é ¼åº¦é …ç›®ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰",
    )

    args = parser.parse_args()

    load_dotenv()

    api_key = args.api_key
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        api_key = input("OpenAI API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        if not api_key:
            print("API KeyãŒå¿…è¦ã§ã™ã€‚")
            print("ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„:")
            print("1. --api-key ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®š")
            print("2. .envãƒ•ã‚¡ã‚¤ãƒ«ã« OPENAI_API_KEY=your_key ã‚’è¨­å®š")
            print("3. å¯¾è©±çš„ã«å…¥åŠ›")
            return

    # Initialize OpenAI client
    try:
        client = openai.OpenAI(api_key=api_key)
        print("OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"API Key ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return

    input_file = args.input
    if not input_file:
        input_file = select_csv_file()
        if not input_file:
            return

    try:
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_file}")
        df = pd.read_csv(input_file)

        # Validate columns
        required_columns = ["å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å", "æ•°"]
        if not all(col in df.columns for col in required_columns):
            print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªåˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“: {required_columns}")
            return

        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)} è¡Œ")

        # Classification
        use_confidence = not args.no_confidence
        print("åˆ†é¡ã‚’é–‹å§‹ã—ã¾ã™...")

        start_time = time.time()
        classified_df = process_csv_with_confidence(
            client, df.copy(), args.batch_size, use_confidence
        )
        processing_time = time.time() - start_time

        if "åˆ†é¡" in classified_df.columns:
            print("åˆ†é¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

            # Create date directory
            date_dir = create_date_directory()

            # Generate output filename with date
            if args.output:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ã—ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«é…ç½®
                output_filename = os.path.basename(args.output)
                output_file = os.path.join(date_dir, output_filename)
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                if use_confidence:
                    output_filename = (
                        f"classified_attributes_with_confidence_{timestamp}.csv"
                    )
                else:
                    output_filename = f"classified_attributes_{timestamp}.csv"
                output_file = os.path.join(date_dir, output_filename)

            # Save results
            classified_df.to_csv(output_file, index=False)
            print(f"åˆ†é¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

            # Show summary
            classification_counts = classified_df["åˆ†é¡"].value_counts()
            print("\nåˆ†é¡çµæœã‚µãƒãƒªãƒ¼:")
            for category, count in classification_counts.items():
                print(f"  {category}: {count}ä»¶")

            # Confidence analysis (if available)
            if use_confidence and "ä¿¡é ¼åº¦" in classified_df.columns:
                confidence_analysis = analyze_confidence_distribution(classified_df)

                # Display confidence analysis
                if confidence_analysis:
                    stats = confidence_analysis.get("stats", {})
                    ranges = confidence_analysis.get("ranges", [])

                    print("\nä¿¡é ¼åº¦åˆ†æ:")
                    for key, value in stats.items():
                        print(f"  {key}: {value:.3f}")

                    print("\nä¿¡é ¼åº¦åŒºé–“åˆ¥çµ±è¨ˆ:")
                    for range_info in ranges:
                        print(
                            f"  {range_info['ä¿¡é ¼åº¦åŒºé–“']}: {range_info['ä»¶æ•°']}ä»¶ ({range_info['å‰²åˆ']:.1f}%)"
                        )

                # Extract and save low confidence items
                low_confidence_items = extract_low_confidence_items(
                    classified_df, args.confidence_threshold
                )
                if not low_confidence_items.empty:
                    low_conf_filename = output_filename.replace(
                        ".csv", "_low_confidence.csv"
                    )
                    low_conf_file = os.path.join(date_dir, low_conf_filename)
                    low_confidence_items.to_csv(low_conf_file, index=False)
                    print(
                        f"\nä½ä¿¡é ¼åº¦é …ç›®ï¼ˆ< {args.confidence_threshold}ï¼‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {low_conf_file}"
                    )
                    print(f"æ¤œè¨¼æ¨å¥¨é …ç›®: {len(low_confidence_items)}ä»¶")

                    # Show top low confidence items
                    if len(low_confidence_items) > 0:
                        print("\nä¿¡é ¼åº¦ãŒæœ€ã‚‚ä½ã„é …ç›®ï¼ˆä¸Šä½5ä»¶ï¼‰:")
                        top_low_confidence = low_confidence_items.head(5)
                        for _, row in top_low_confidence.iterrows():
                            print(
                                f"  - {row['å±æ€§ã‚°ãƒ«ãƒ¼ãƒ—å']} (åˆ†é¡: {row['åˆ†é¡']}, ä¿¡é ¼åº¦: {row['ä¿¡é ¼åº¦']:.3f})"
                            )

                # Generate and save markdown report
                if confidence_analysis:
                    report = generate_markdown_report(
                        classified_df,
                        output_file,
                        confidence_analysis,
                        low_confidence_items,
                        processing_time,
                    )
                    report_file = save_markdown_report(report, output_file, date_dir)
                    print(f"\nğŸ“Š ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_file}")

        else:
            print("åˆ†é¡å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


if __name__ == "__main__":
    main()
